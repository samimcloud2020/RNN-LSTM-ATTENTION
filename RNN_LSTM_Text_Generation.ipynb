{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WT7OYVqgMe4t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def split_text_file(input_file, output_file1, output_file2, split_ratio=0.01):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    split_index = int(len(lines) * split_ratio)  # Compute 20% split index\n",
        "\n",
        "    # Write the first 20% to output_file1\n",
        "    with open(output_file1, \"w\", encoding=\"utf-8\") as file1:\n",
        "        file1.writelines(lines[:split_index])\n",
        "\n",
        "    # Write the remaining 80% to output_file2\n",
        "    with open(output_file2, \"w\", encoding=\"utf-8\") as file2:\n",
        "        file2.writelines(lines[split_index:])\n",
        "\n",
        "    print(f\"✅ File successfully split into '{output_file1}' (5%) and '{output_file2}' (95%)\")\n",
        "\n",
        "# Example Usage\n",
        "split_text_file(\"shakespeare.txt\", \"output_05.txt\", \"output_95.txt\")\n",
        "\n",
        "\n",
        "# Load dataset (Shakespeare's text as an example)\n",
        "with open(\"output_05.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()  # Convert to lowercase for consistency\n",
        "\n",
        "# Create character-to-index mapping\n",
        "chars = sorted(set(text))\n",
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "\n",
        "# Convert text to numbers\n",
        "def text_to_sequences(text, seq_length):\n",
        "    input_sequences = []\n",
        "    target_sequences = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        input_sequences.append([char_to_idx[char] for char in text[i:i+seq_length]])\n",
        "        target_sequences.append(char_to_idx[text[i+seq_length]])\n",
        "    return np.array(input_sequences), np.array(target_sequences)\n",
        "\n",
        "SEQ_LENGTH = 100  # Length of input sequences\n",
        "X, Y = text_to_sequences(text, SEQ_LENGTH)\n",
        "\n",
        "# Normalize data (convert to float and scale)\n",
        "vocab_size = len(chars)\n",
        "X = tf.keras.utils.to_categorical(X, num_classes=vocab_size)  # One-hot encode input\n",
        "Y = tf.keras.utils.to_categorical(Y, num_classes=vocab_size)\n",
        "\n",
        "# Define the RNN Model with LSTM\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(256, input_shape=(SEQ_LENGTH, vocab_size), return_sequences=True),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "#model.fit(X, Y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "# Train model with validation split (80% training, 20% validation)\n",
        "model.fit(X, Y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)\n",
        "\n",
        "# Save model\n",
        "model.save(\"text_generator_rnn.h5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PorYHOXSMijT",
        "outputId": "596a31bc-3a23-42ed-8d77-d4c5b3c6ebe3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ File successfully split into 'output_05.txt' (5%) and 'output_95.txt' (95%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 25ms/step - accuracy: 0.2306 - loss: 2.9019 - val_accuracy: 0.3860 - val_loss: 2.0950\n",
            "Epoch 2/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.3866 - loss: 2.1102 - val_accuracy: 0.4489 - val_loss: 1.8577\n",
            "Epoch 3/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.4557 - loss: 1.8372 - val_accuracy: 0.4873 - val_loss: 1.7376\n",
            "Epoch 4/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.5079 - loss: 1.6705 - val_accuracy: 0.4893 - val_loss: 1.6814\n",
            "Epoch 5/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 23ms/step - accuracy: 0.5498 - loss: 1.4935 - val_accuracy: 0.5133 - val_loss: 1.6391\n",
            "Epoch 6/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.5910 - loss: 1.3414 - val_accuracy: 0.5095 - val_loss: 1.6458\n",
            "Epoch 7/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.6260 - loss: 1.2086 - val_accuracy: 0.5094 - val_loss: 1.6697\n",
            "Epoch 8/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 25ms/step - accuracy: 0.6617 - loss: 1.0868 - val_accuracy: 0.5075 - val_loss: 1.7038\n",
            "Epoch 9/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.7067 - loss: 0.9378 - val_accuracy: 0.4993 - val_loss: 1.7792\n",
            "Epoch 10/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.7256 - loss: 0.8855 - val_accuracy: 0.5102 - val_loss: 1.7833\n",
            "Epoch 11/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.7591 - loss: 0.7748 - val_accuracy: 0.4965 - val_loss: 1.9039\n",
            "Epoch 12/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.8161 - loss: 0.6061 - val_accuracy: 0.4934 - val_loss: 2.0282\n",
            "Epoch 13/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.8467 - loss: 0.5186 - val_accuracy: 0.4883 - val_loss: 2.1286\n",
            "Epoch 14/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.8688 - loss: 0.4440 - val_accuracy: 0.4949 - val_loss: 2.2266\n",
            "Epoch 15/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 23ms/step - accuracy: 0.8913 - loss: 0.3706 - val_accuracy: 0.4803 - val_loss: 2.3490\n",
            "Epoch 16/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 25ms/step - accuracy: 0.8983 - loss: 0.3366 - val_accuracy: 0.4784 - val_loss: 2.4695\n",
            "Epoch 17/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 23ms/step - accuracy: 0.9159 - loss: 0.2900 - val_accuracy: 0.4780 - val_loss: 2.5881\n",
            "Epoch 18/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 25ms/step - accuracy: 0.9278 - loss: 0.2444 - val_accuracy: 0.4786 - val_loss: 2.6968\n",
            "Epoch 19/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9405 - loss: 0.2081 - val_accuracy: 0.4693 - val_loss: 2.8209\n",
            "Epoch 20/20\n",
            "\u001b[1m622/622\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 25ms/step - accuracy: 0.9277 - loss: 0.2416 - val_accuracy: 0.4798 - val_loss: 2.8402\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(start_text, length=500):\n",
        "    start_text = start_text.lower()  # Ensure lowercase consistency\n",
        "    generated_text = start_text\n",
        "\n",
        "    # Convert seed text into a sequence\n",
        "    input_seq = [char_to_idx[char] for char in start_text]\n",
        "\n",
        "    # Pad input sequence to SEQ_LENGTH\n",
        "    if len(input_seq) < SEQ_LENGTH:\n",
        "        input_seq = [0] * (SEQ_LENGTH - len(input_seq)) + input_seq  # Left-padding\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Prepare input sequence\n",
        "        input_data = tf.keras.utils.to_categorical([input_seq], num_classes=vocab_size)\n",
        "\n",
        "        # Ensure correct shape\n",
        "        input_data = np.reshape(input_data, (1, SEQ_LENGTH, vocab_size))\n",
        "\n",
        "        # Predict next character\n",
        "        predicted_probs = model.predict(input_data, verbose=0)\n",
        "        predicted_idx = np.argmax(predicted_probs)\n",
        "\n",
        "        # Append character to generated text\n",
        "        next_char = idx_to_char[predicted_idx]\n",
        "        generated_text += next_char\n",
        "\n",
        "        # Update input sequence\n",
        "        input_seq.append(predicted_idx)\n",
        "        input_seq = input_seq[1:]  # Keep sequence length constant\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using a seed phrase\n",
        "print(generate_text(\"pizza eat\", 500))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfDc5EF9SGlE",
        "outputId": "1e51dce1-59e6-44c6-d601-f96d0ff64824"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pizza eather new least of the fierce to the extext or any of the future is subject touch complete,\n",
            "inaccurs, and earthed berit of recdies,\n",
            "  and loathsome canker lives in sweete to grace,\n",
            "  and summer's lease hath despite twine reclive,\n",
            "    theirs for their style i'll read, and namy prousulled:\n",
            "  then i ab presagere's distillation left\n",
            "  and sable the orient when the gracious light\n",
            "  lifts up his burning his grace, in wanting wast,\n",
            "  and do whate'er thou wilt swift-footed time\n",
            "  to the wide world and all\n"
          ]
        }
      ]
    }
  ]
}