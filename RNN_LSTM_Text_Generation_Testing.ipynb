{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC7xPnfFoT4v",
        "outputId": "0b202f08-205a-4896-c710-89fedc43b9c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… File successfully split into 'output_20.txt' (20%) and 'output_80.txt' (80%)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "def split_text_file(input_file, output_file1, output_file2, split_ratio=0.001):\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    split_index = int(len(lines) * split_ratio)  # Compute 20% split index\n",
        "\n",
        "    # Write the first 20% to output_file1\n",
        "    with open(output_file1, \"w\", encoding=\"utf-8\") as file1:\n",
        "        file1.writelines(lines[:split_index])\n",
        "\n",
        "    # Write the remaining 80% to output_file2\n",
        "    with open(output_file2, \"w\", encoding=\"utf-8\") as file2:\n",
        "        file2.writelines(lines[split_index:])\n",
        "\n",
        "    print(f\"âœ… File successfully split into '{output_file1}' (20%) and '{output_file2}' (80%)\")\n",
        "\n",
        "# Example Usage\n",
        "split_text_file(\"shakespeare.txt\", \"output_20.txt\", \"output_80.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset (Shakespeare's text as an example)\n",
        "with open(\"output_20.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    text = f.read().lower()  # Convert to lowercase for consistency\n"
      ],
      "metadata": {
        "id": "jBSmM6b4pNmK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "Yico8u3-pNo9",
        "outputId": "b8c059e8-c309-41cd-ae44-96ed2d636b39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is the 100th etext file presented by project gutenberg, and\\nis presented in cooperation with world library, inc., from their\\nlibrary of the future and shakespeare cdroms.  project gutenberg\\noften releases etexts that are not placed in the public domain!!\\n\\nshakespeare\\n\\n*this etext has certain copyright implications you should read!*\\n\\n<<this electronic version of the complete works of william\\nshakespeare is copyright 1990-1993 by world library, inc., and is\\nprovided by project gutenberg etext of illinois benedictine college\\nwith permission.  electronic and machine readable copies may be\\ndistributed so long as such copies (1) are for your or others\\npersonal use only, and (2) are not distributed or used\\ncommercially.  prohibited commercial distribution includes by any\\nservice that charges for download time or for membership.>>\\n\\n*project gutenberg is proud to cooperate with the world library*\\nin the presentation of the complete works of william shakespeare\\nfor your reading for education and entertainment.  however, this\\nis neither shareware nor public domain. . .and under the library\\nof the future conditions of this presentation. . .no charges may\\nbe made for *any* access to this material.  you are encouraged!!\\nto give it away to anyone you like, but no charges are allowed!!\\n\\n\\n**welcome to the world of free plain vanilla electronic texts**\\n\\n**etexts readable by both humans and by computers, since 1971**\\n\\n*these etexts prepared by hundreds of volunteers and donations*\\n\\ninformation on contacting project gutenberg to get etexts, and\\nfurther information is included below.  we need your donations.\\n\\n\\nthe complete works of william shakespeare \\n\\njanuary, 1994  [etext #100]\\n\\n\\nthe library of the future complete works of william shakespeare \\nlibrary of the future is a trademark (tm) of world library inc.\\n******this file should be named shaks12.txt or shaks12.zip*****\\n\\ncorrected editions of our etexts get a new number, shaks13.txt\\nversions based on separate sources get new letter, shaks10a.txt\\n\\nif you would like further information about world library, inc.\\nplease call them at 1-800-443-0238 or email julianc@netcom.com\\nplease give them our thanks for their shakespeare cooperation!\\n\\n\\nthe official release date of all project gutenberg etexts is at\\nmidnight, central time, of the last day of the stated month.  a\\npreliminary version may often be posted for suggestion, comment\\nand editing by those who wish to do so.  to be sure you have an\\nup to date first edition [xxxxx10x.xxx] please check file sizes\\nin the first week of the next month.  since our ftp program has\\na bug in it that scrambles the date [tried to fix and failed] a\\nlook at the file size will have to do, but we will try to see a\\nnew copy has at least one byte more or less.\\n\\n\\ninformation about project gutenberg (one page)\\n\\nwe produce about two million dollars for each hour we work.  the\\nfifty hours is one conservative estimate for how long it we take\\nto get any etext selected, entered, proofread, edited, copyright\\nsearched and analyzed, the copyright letters written, etc.  this\\nprojected audience is one hundred million readers.  if our value\\nper text is nominally estimated at one dollar, then we produce 2\\nmillion dollars per hour this year we, will have to do four text\\nfiles per month:  thus upping our productivity from one million.\\nthe goal of project gutenberg is to give away one trillion etext\\nfiles by the december 31, 2001.  [10,000 x 100,000,000=trillion]\\nthis is ten thousand titles each to one hundred million readers,\\nwhich is 10% of the expected number of computer users by the end\\nof the year 2001.\\n\\nwe need your donations more than ever!\\n\\nall donations should be made to \"project gutenberg/ibc\", and are\\ntax deductible to the extent allowable by law (\"ibc\" is illinois\\nbenedictine college).  (subscriptions to our paper newsletter go\\nto ibc, too)\\n\\nfor these and other matters, please mail to:\\n\\nproject gutenberg\\np. o. box  2782\\nchampaign, il 61825\\n\\nwhen all other email fails try our michael s. hart, executive director:\\nhart@vmd.cso.uiuc.edu (internet)   hart@uiucvmd   (bitnet)\\n\\nwe would prefer to send you this information by email\\n(internet, bitnet, compuserve, attmail or mcimail).\\n\\n******\\nif you have an ftp program (or emulator), please\\nftp directly to the project gutenberg archives:\\n[mac users, do not point and click. . .type]\\n\\nftp mrcnext.cso.uiuc.edu\\nlogin:  anonymous\\npassword:  your@login\\ncd etext/etext91\\nor cd etext92\\nor cd etext93 [for new books]  [now also in cd etext/etext93]\\nor cd etext/articles [get suggest gut for more information]\\ndir [to see files]\\nget or mget [to get files. . .set bin for zip files]\\nget 0index.gut\\nfor a list of books\\nand\\nget new gut for general information\\nand\\nmget gut* for newsletters.\\n\\n**information prepared by the project gutenberg legal advisor**\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create character-to-index mapping\n",
        "chars = sorted(set(text))"
      ],
      "metadata": {
        "id": "w9Fa49XSpNry"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8oLOH_rupNu6",
        "outputId": "ec375acd-ecd3-4a28-c6ea-52995c8cd39c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '#',\n",
              " '%',\n",
              " '(',\n",
              " ')',\n",
              " '*',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '/',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " '<',\n",
              " '=',\n",
              " '>',\n",
              " '@',\n",
              " '[',\n",
              " ']',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx = {char: idx for idx, char in enumerate(chars)}\n",
        "char_to_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKgxSLdbpNx1",
        "outputId": "5c20fb88-eab0-4f76-cc4c-86ca739ed29a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'\\n': 0,\n",
              " ' ': 1,\n",
              " '!': 2,\n",
              " '\"': 3,\n",
              " '#': 4,\n",
              " '%': 5,\n",
              " '(': 6,\n",
              " ')': 7,\n",
              " '*': 8,\n",
              " ',': 9,\n",
              " '-': 10,\n",
              " '.': 11,\n",
              " '/': 12,\n",
              " '0': 13,\n",
              " '1': 14,\n",
              " '2': 15,\n",
              " '3': 16,\n",
              " '4': 17,\n",
              " '5': 18,\n",
              " '6': 19,\n",
              " '7': 20,\n",
              " '8': 21,\n",
              " '9': 22,\n",
              " ':': 23,\n",
              " '<': 24,\n",
              " '=': 25,\n",
              " '>': 26,\n",
              " '@': 27,\n",
              " '[': 28,\n",
              " ']': 29,\n",
              " 'a': 30,\n",
              " 'b': 31,\n",
              " 'c': 32,\n",
              " 'd': 33,\n",
              " 'e': 34,\n",
              " 'f': 35,\n",
              " 'g': 36,\n",
              " 'h': 37,\n",
              " 'i': 38,\n",
              " 'j': 39,\n",
              " 'k': 40,\n",
              " 'l': 41,\n",
              " 'm': 42,\n",
              " 'n': 43,\n",
              " 'o': 44,\n",
              " 'p': 45,\n",
              " 'r': 46,\n",
              " 's': 47,\n",
              " 't': 48,\n",
              " 'u': 49,\n",
              " 'v': 50,\n",
              " 'w': 51,\n",
              " 'x': 52,\n",
              " 'y': 53,\n",
              " 'z': 54}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx_to_char = {idx: char for idx, char in enumerate(chars)}\n",
        "idx_to_char"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_kqjaxepN0k",
        "outputId": "74080746-9502-4c9c-f286-f695bfb73f6c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: '\\n',\n",
              " 1: ' ',\n",
              " 2: '!',\n",
              " 3: '\"',\n",
              " 4: '#',\n",
              " 5: '%',\n",
              " 6: '(',\n",
              " 7: ')',\n",
              " 8: '*',\n",
              " 9: ',',\n",
              " 10: '-',\n",
              " 11: '.',\n",
              " 12: '/',\n",
              " 13: '0',\n",
              " 14: '1',\n",
              " 15: '2',\n",
              " 16: '3',\n",
              " 17: '4',\n",
              " 18: '5',\n",
              " 19: '6',\n",
              " 20: '7',\n",
              " 21: '8',\n",
              " 22: '9',\n",
              " 23: ':',\n",
              " 24: '<',\n",
              " 25: '=',\n",
              " 26: '>',\n",
              " 27: '@',\n",
              " 28: '[',\n",
              " 29: ']',\n",
              " 30: 'a',\n",
              " 31: 'b',\n",
              " 32: 'c',\n",
              " 33: 'd',\n",
              " 34: 'e',\n",
              " 35: 'f',\n",
              " 36: 'g',\n",
              " 37: 'h',\n",
              " 38: 'i',\n",
              " 39: 'j',\n",
              " 40: 'k',\n",
              " 41: 'l',\n",
              " 42: 'm',\n",
              " 43: 'n',\n",
              " 44: 'o',\n",
              " 45: 'p',\n",
              " 46: 'r',\n",
              " 47: 's',\n",
              " 48: 't',\n",
              " 49: 'u',\n",
              " 50: 'v',\n",
              " 51: 'w',\n",
              " 52: 'x',\n",
              " 53: 'y',\n",
              " 54: 'z'}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text) - 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYbm-JuErAiA",
        "outputId": "2ab47fe9-d6a2-41f4-ccc7-ccfa6867ffd3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4718"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[0:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8WmK7tTPrWdo",
        "outputId": "8bdb9b1c-6bff-4770-a646-f8fa31eb5e3a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'this is the 100th etext file presented by project gutenberg, and\\nis presented in cooperation with wo'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text[100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_JAV9jmerrMh",
        "outputId": "4d91cd2e-5ab3-4991-d905-79c223e64f82"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'r'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_idx[\"t\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuFwSAlpyYba",
        "outputId": "e2254758-820b-442f-9802-eae05bd1ed9b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "48"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to numbers\n",
        "def text_to_sequences(text, seq_length):\n",
        "    input_sequences = []\n",
        "    target_sequences = []\n",
        "    for i in range(len(text) - seq_length):\n",
        "        input_sequences.append([char_to_idx[char] for char in text[i:i+seq_length]])\n",
        "        target_sequences.append(char_to_idx[text[i+seq_length]])\n",
        "        #print(\"*****************input seq*********************************************\")\n",
        "        #print(input_sequences)\n",
        "        #print(f\"The length of input seq is {len(input_sequences)}\")\n",
        "        #print(\"*****************target seq*********************************************\")\n",
        "        #print(target_sequences)\n",
        "        #print(f\"The length of target seq is {len(target_sequences)}\")\n",
        "        #print(\"--------------\")\n",
        "    return np.array(input_sequences), np.array(target_sequences)\n",
        "\n",
        "SEQ_LENGTH = 100  # Length of input sequences\n",
        "X, Y = text_to_sequences(text, SEQ_LENGTH)"
      ],
      "metadata": {
        "id": "MjMCR-hyqwTJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X),len(Y)"
      ],
      "metadata": {
        "id": "EfFN4L6-qwV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b63850d-af4b-41b4-e47e-e0205cbfba19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4718, 4718)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize data (convert to float and scale)\n",
        "vocab_size = len(chars)\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "9uA4xVENqwYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7d4938-980d-43c5-8dd4-d8cc5960046f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = tf.keras.utils.to_categorical(X, num_classes=vocab_size)  # One-hot encode input\n",
        "X"
      ],
      "metadata": {
        "id": "SWUSv4ioqwav",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "245c71dd-6eca-4511-b3ba-bd05914ecb89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 1., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.],\n",
              "        [1., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X)"
      ],
      "metadata": {
        "id": "9RwsFtROqwcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f6e5722-e8e6-4fbc-fc2e-bc71c8938f32"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4718"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y = tf.keras.utils.to_categorical(Y, num_classes=vocab_size)"
      ],
      "metadata": {
        "id": "huJCUxYyqwfD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Why stack LSTMs?\n",
        "Stacking multiple LSTM layers allows the model to learn more complex features and patterns. The first LSTM layer captures low-level features of the sequence, while the second LSTM layer can combine those features into higher-level representations.\n",
        "\n",
        "ğŸ”¹ Summary of the Flow:\n",
        "\n",
        "First LSTM layer processes the input sequence and outputs the hidden states for each time step.\n",
        "\n",
        "Second LSTM layer processes the hidden states from the first LSTM and outputs only the final hidden state.\n",
        "\n",
        "The final hidden state is passed through a Dense layer to produce a vector of probabilities, where each value corresponds to the probability of a specific character in the vocabulary."
      ],
      "metadata": {
        "id": "v_Jqo9cy3uVz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RNN Model with LSTM\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.LSTM(256, input_shape=(SEQ_LENGTH, vocab_size), return_sequences=True),\n",
        "    tf.keras.layers.LSTM(256),\n",
        "    tf.keras.layers.Dense(vocab_size, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=0.002), metrics=[\"accuracy\"])\n",
        "\n",
        "# Train model\n",
        "EPOCHS = 20\n",
        "BATCH_SIZE = 64\n",
        "#model.fit(X, Y, batch_size=BATCH_SIZE, epochs=EPOCHS)\n",
        "# Train model with validation split (80% training, 20% validation)\n",
        "model.fit(X, Y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2)"
      ],
      "metadata": {
        "id": "XhQ3dl1_qwhN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d87ffee0-2864-42fe-cea7-f5d2d6546d43"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 1s/step - accuracy: 0.1320 - loss: 3.3772 - val_accuracy: 0.1356 - val_loss: 3.3317\n",
            "Epoch 2/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.1488 - loss: 3.1407 - val_accuracy: 0.1186 - val_loss: 3.2888\n",
            "Epoch 3/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.1812 - loss: 3.0405 - val_accuracy: 0.1716 - val_loss: 3.1574\n",
            "Epoch 4/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.2426 - loss: 2.7463 - val_accuracy: 0.2066 - val_loss: 3.0447\n",
            "Epoch 5/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - accuracy: 0.2972 - loss: 2.5092 - val_accuracy: 0.2256 - val_loss: 2.9743\n",
            "Epoch 6/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.3476 - loss: 2.2906 - val_accuracy: 0.2701 - val_loss: 2.8638\n",
            "Epoch 7/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.3882 - loss: 2.0992 - val_accuracy: 0.3030 - val_loss: 2.8472\n",
            "Epoch 8/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.4586 - loss: 1.9137 - val_accuracy: 0.2850 - val_loss: 2.8478\n",
            "Epoch 9/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 1s/step - accuracy: 0.5091 - loss: 1.7043 - val_accuracy: 0.3104 - val_loss: 2.8587\n",
            "Epoch 10/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m74s\u001b[0m 1s/step - accuracy: 0.5448 - loss: 1.5766 - val_accuracy: 0.3104 - val_loss: 2.8987\n",
            "Epoch 11/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 1s/step - accuracy: 0.5888 - loss: 1.3865 - val_accuracy: 0.3061 - val_loss: 2.8972\n",
            "Epoch 12/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 1s/step - accuracy: 0.6260 - loss: 1.2550 - val_accuracy: 0.3061 - val_loss: 3.1049\n",
            "Epoch 13/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 1s/step - accuracy: 0.6915 - loss: 1.0396 - val_accuracy: 0.3019 - val_loss: 3.1485\n",
            "Epoch 14/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 1s/step - accuracy: 0.7591 - loss: 0.8521 - val_accuracy: 0.2998 - val_loss: 3.3170\n",
            "Epoch 15/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.8022 - loss: 0.7107 - val_accuracy: 0.2881 - val_loss: 3.4285\n",
            "Epoch 16/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 1s/step - accuracy: 0.8538 - loss: 0.5654 - val_accuracy: 0.3008 - val_loss: 3.5936\n",
            "Epoch 17/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 1s/step - accuracy: 0.8811 - loss: 0.4663 - val_accuracy: 0.2966 - val_loss: 3.7754\n",
            "Epoch 18/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.9301 - loss: 0.3168 - val_accuracy: 0.2892 - val_loss: 3.8605\n",
            "Epoch 19/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 1s/step - accuracy: 0.9622 - loss: 0.2207 - val_accuracy: 0.2881 - val_loss: 4.0181\n",
            "Epoch 20/20\n",
            "\u001b[1m59/59\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 1s/step - accuracy: 0.9767 - loss: 0.1516 - val_accuracy: 0.3019 - val_loss: 4.2065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7df403fa0750>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model.save(\"text_generator_rnn.h5\")"
      ],
      "metadata": {
        "id": "JlVBK63nqwjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf39cb2-ab15-4789-f04c-16250a915872"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(start_text, length=500):\n",
        "    start_text = start_text.lower()  # Ensure lowercase consistency\n",
        "    generated_text = start_text\n",
        "\n",
        "    # Convert seed text into a sequence\n",
        "    input_seq = [char_to_idx[char] for char in start_text]\n",
        "\n",
        "    # Pad input sequence to SEQ_LENGTH\n",
        "    if len(input_seq) < SEQ_LENGTH:\n",
        "        input_seq = [0] * (SEQ_LENGTH - len(input_seq)) + input_seq  # Left-padding\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Prepare input sequence\n",
        "        input_data = tf.keras.utils.to_categorical([input_seq], num_classes=vocab_size)\n",
        "\n",
        "        # Ensure correct shape\n",
        "        input_data = np.reshape(input_data, (1, SEQ_LENGTH, vocab_size))\n",
        "\n",
        "        # Predict next character\n",
        "        predicted_probs = model.predict(input_data, verbose=0)\n",
        "        predicted_idx = np.argmax(predicted_probs)\n",
        "\n",
        "        # Append character to generated text\n",
        "        next_char = idx_to_char[predicted_idx]\n",
        "        generated_text += next_char\n",
        "\n",
        "        # Update input sequence\n",
        "        input_seq.append(predicted_idx)\n",
        "        input_seq = input_seq[1:]  # Keep sequence length constant\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using a seed phrase\n",
        "print(generate_text(\"pizza eat\", 500))"
      ],
      "metadata": {
        "id": "O9fgCR9Rqwla",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a77510-6d2a-4b78-d819-d753690a05bf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pizza eato tat is at one toned more to les are to \"project gutenberg (one page)\n",
            "\n",
            "we produce about two million dollars for each hour we work.  the\n",
            "fifty hours is one conservative estimate for how long it we take\n",
            "to get any etext selected, entered, proofread, edited, copyright\n",
            "searched and analyzed, the copyright letters written, etc.  this\n",
            "projected audience is one hundred million readers, which is 10% of the future is a trademark (tm) of world library, inc., and and by charses!!\n",
            "\n",
            "the goan of project gute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(start_text, length=500):\n",
        "    start_text = start_text.lower()  # Ensure lowercase consistency\n",
        "    generated_text = start_text\n",
        "\n",
        "    # Convert seed text into a sequence\n",
        "    input_seq = [char_to_idx[char] for char in start_text]\n",
        "\n",
        "    # Pad input sequence to SEQ_LENGTH\n",
        "    if len(input_seq) < SEQ_LENGTH:\n",
        "        input_seq = [0] * (SEQ_LENGTH - len(input_seq)) + input_seq  # Left-padding\n",
        "\n",
        "    for _ in range(length):\n",
        "        # Prepare input sequence\n",
        "        input_data = tf.keras.utils.to_categorical([input_seq], num_classes=vocab_size)\n",
        "\n",
        "        # Ensure correct shape\n",
        "        input_data = np.reshape(input_data, (1, SEQ_LENGTH, vocab_size))\n",
        "\n",
        "        # Predict next character\n",
        "        predicted_probs = model.predict(input_data, verbose=0)\n",
        "        predicted_idx = np.argmax(predicted_probs)\n",
        "\n",
        "        # Append character to generated text\n",
        "        next_char = idx_to_char[predicted_idx]\n",
        "        generated_text += next_char\n",
        "\n",
        "        # Update input sequence\n",
        "        input_seq.append(predicted_idx)\n",
        "        input_seq = input_seq[1:]  # Keep sequence length constant\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "# Generate text using a seed phrase\n",
        "print(generate_text(\"i love you\", 500))"
      ],
      "metadata": {
        "id": "SVaqPmu_qwnS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c77e00-5b66-4844-bb30-80280d0d8227"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i love your donations more than ever!\n",
            "\n",
            "all donations should be made to \"project gutenberg etexts is at\n",
            "midnight, central time, of the last day of the stated month.  a\n",
            "preliminary version may often be posted for suggestion, comment\n",
            "and editing by those who wish to do so bec wess\n",
            "for your or others\n",
            "personal use comperate with the world library, inc., and and by charses!!\n",
            "\n",
            "the goan of project gutenberg (one page)\n",
            "\n",
            "we produce about two million dollars for each hour we work.  the\n",
            "fifty hours is one conservativ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zy5OEMaJqwpT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfYIWJybqwrd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5_y--R4iqwth"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4cVab7j8qwwY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oD4H9TPbpN3k"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}